{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfb0d10-7a1c-430a-b509-f4df098c6af8",
   "metadata": {},
   "source": [
    "# **IE7275 Data Mining in Engineering**\n",
    "# **Fall 2025 semester**\n",
    "### -- STUDENT VERSION --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbd6f2-87e7-40db-b9a1-1578f56d4a33",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Guidelines for Completing and Submitting This Notebook**\n",
    "\n",
    "Please follow these instructions carefully. Completing all parts of this notebook is required to receive full credit:\n",
    "\n",
    "#### **You must:**\n",
    "- **Answer all questions** and reflection tasks using your own words.\n",
    "- **Summarize every reflection task** clearly and completely.\n",
    "- **Fill out all missing code cells** — do not leave any code blocks empty.\n",
    "- **Run your notebook** to ensure that all outputs and visualizations are generated and visible.\n",
    "- **Convert your completed notebook** to **PDF** or **HTML** format before submission.\n",
    "- **Submit the file to Canvas** before the deadline.\n",
    "\n",
    "#### **Academic Integrity Reminder**:\n",
    "- You must complete this notebook individually.\n",
    "- Do not copy answers or code from classmates, online sources, or use tools like **ChatGPT** or other **AI writing or code generation tools**.\n",
    "- Keep in mind that if you use such tools, your answer may be **identical or highly similar** to others who do the same — in this case, we will treat it as copying and apply a **50% penalty** to your assignment grade.\n",
    "\n",
    "By submitting this notebook, you confirm that all work is your own and that you have followed these guidelines.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16f511-6175-4c86-a4a0-e614217a6647",
   "metadata": {},
   "source": [
    "## **Module 4: Association Rule Mining and Recommendation Systems (Part 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dff90-0b5f-40a7-a5e7-93d4dc2ad3c2",
   "metadata": {},
   "source": [
    "This module explores techniques for discovering interesting relationships and patterns in large datasets through **association rule mining** and applying these insights to build **recommendation systems**.\n",
    "\n",
    "You will learn how to analyze transactional data, extract frequent itemsets using algorithms like Apriori and FP-Growth, and evaluate rules using metrics such as support, confidence, and lift. The module also covers advanced topics like rule constraints, visualization of association patterns, and handling complex multi-level rules.\n",
    "\n",
    "In addition, you will study recommendation system fundamentals, including association-based recommenders, collaborative filtering, and hybrid approaches that combine multiple methods for improved accuracy.\n",
    "\n",
    "By the end of this module, you will be able to mine meaningful associations from data and leverage them to design effective recommendation engines used widely in e-commerce, marketing, and user personalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514c25b-70d7-4021-93b9-05199479dede",
   "metadata": {},
   "source": [
    "#### **Module 4.1: Introduction to Association Rule Mining**\n",
    "\n",
    "**Association Rule Mining** is a data mining technique used to discover meaningful relationships, patterns, and correlations among items in large datasets, typically transactional data. The primary goal is to generate **rules** that predict the occurrence of one or more items based on the presence of other items. This is widely used in market basket analysis, recommendation systems, and customer behavior analytics.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Association Rules Matter**\n",
    "Association rules help uncover hidden patterns that are not obvious from raw data.  \n",
    "For example, in retail:\n",
    "> Customers who buy **bread** and **butter** are also likely to buy **jam**.\n",
    "\n",
    "Such insights can guide:\n",
    "- **Product Placement** — placing complementary products near each other.\n",
    "- **Cross-Selling** — recommending related products to customers.\n",
    "- **Marketing Campaigns** — creating targeted promotions for specific item combinations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Concepts and Definitions**\n",
    "\n",
    "1. **Itemset**  \n",
    "   - A collection of one or more items.\n",
    "   - **k-itemset**: An itemset containing exactly $(k)$ items.  \n",
    "     Example: $(\\{ \\text{bread}, \\text{butter} \\})$ is a 2-itemset.\n",
    "\n",
    "2. **Support ($ \\text{supp}(A) $)**  \n",
    "   - The proportion of transactions that contain the itemset $(A)$.  \n",
    "   - Formula:  \n",
    "     $\n",
    "     \\text{Support}(A) = \\frac{\\text{Number of transactions containing } A}{\\text{Total number of transactions}}\n",
    "     $  \n",
    "   - Measures **how frequently** an itemset appears in the dataset.\n",
    "\n",
    "3. **Confidence ($ \\text{conf}(A \\rightarrow B) $)**  \n",
    "   - The probability of finding itemset $(B)$ in transactions that contain $(A)$.  \n",
    "   - Formula:  \n",
    "     $\n",
    "     \\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A)}\n",
    "     $  \n",
    "   - Indicates the **reliability** of the rule.\n",
    "\n",
    "4. **Lift ($ \\text{lift}(A \\rightarrow B) $)**  \n",
    "   - The ratio of the observed support of $(A)$ and $(B)$ appearing together to the expected support if they were independent.  \n",
    "   - Formula:  \n",
    "     $\n",
    "     \\text{Lift}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A) \\times \\text{Support}(B)}\n",
    "     $  \n",
    "   - **Lift > 1**: Positive association (items occur together more often than expected by chance).  \n",
    "   - **Lift = 1**: No association (independent).  \n",
    "   - **Lift < 1**: Negative association (items occur together less often than expected).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example Interpretation**\n",
    "Rule: $(\\{\\text{Bread}, \\text{Butter}\\} \\rightarrow \\{\\text{Jam}\\})$  \n",
    "- **Support**: 5% — This rule applies to 5% of all transactions.  \n",
    "- **Confidence**: 60% — In 60% of cases where bread and butter are bought together, jam is also bought.  \n",
    "- **Lift**: 2.0 — Buying bread and butter together makes jam purchases twice as likely compared to random chance.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Process in Association Rule Mining**\n",
    "1. **Identify Frequent Itemsets** — Use algorithms like Apriori, FP-Growth, or Eclat to find itemsets above a minimum support threshold.\n",
    "2. **Generate Rules** — From frequent itemsets, create rules that meet minimum confidence criteria.\n",
    "3. **Evaluate Rules** — Use metrics such as support, confidence, and lift to determine rule usefulness.\n",
    "4. **Apply Insights** — Integrate the rules into decision-making, marketing, and product recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Important Considerations**\n",
    "- Association rules **do not imply causation** — they only describe statistical relationships.\n",
    "- Choosing appropriate **support** and **confidence** thresholds is essential to balance rule quantity and quality.\n",
    "- Very high support may ignore rare but valuable patterns; very low support may produce too many trivial or noisy rules.\n",
    "- Domain knowledge is crucial to ensure discovered rules are actionable and relevant.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407a5b2-b53e-45c9-8706-6d9c768716b0",
   "metadata": {},
   "source": [
    "#### **Exercise 1: Exploring Basic Metrics of Association Rules**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to extract frequent itemsets and calculate the basic metrics of association rules—support, confidence, and lift—using a transactional dataset.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Load the Dataset**  \n",
    "   Use the built-in **Online Retail** dataset or simulate a small transactional dataset. Each transaction should consist of a list of items purchased together.\n",
    "\n",
    "2. **Preprocess the Data**  \n",
    "   - Convert the dataset into a format suitable for association rule mining (i.e., a binary-encoded DataFrame where rows represent transactions and columns represent items).\n",
    "   - Use the `TransactionEncoder` from `mlxtend.preprocessing`.\n",
    "\n",
    "3. **Generate Frequent Itemsets**  \n",
    "   - Apply the **Apriori algorithm** using `mlxtend.frequent_patterns.apriori()` to identify frequent itemsets with a minimum support of 0.2.\n",
    "\n",
    "4. **Generate Association Rules**  \n",
    "   - Use `mlxtend.frequent_patterns.association_rules()` to derive rules from the frequent itemsets.\n",
    "   - Calculate the **support**, **confidence**, and **lift** for each rule.\n",
    "\n",
    "5. **Interpret the Results**  \n",
    "   - Print out the top 5 rules sorted by lift.\n",
    "   - Discuss one rule you find interesting and explain what it means in terms of item relationships.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b927c7a-e071-48aa-8897-13e6c704ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mlxtend if not already installed\n",
    "#!pip install mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8132a43-2510-4027-9380-faf87747b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3de91-bae9-4d31-9f3a-746001979add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a simulated transaction dataset\n",
    "transactions = [\n",
    "    ['bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['jam', 'bread'],\n",
    "    ['milk', 'jam'],\n",
    "    ['bread', 'butter'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread', 'milk', 'butter', 'jam']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104c6a5-49d9-46b8-8159-e74f23b20106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Encode the dataset\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea9516-014a-4b57-96dd-9e89d1f612e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate frequent itemsets using Apriori\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd980c-c2fc-4588-af17-0b80938373cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22318bcb-5e4f-4efa-ac20-24abdb1ca9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Display the top 5 rules by lift\n",
    "top_rules = rules.sort_values(by='lift', ascending=False).head(5)\n",
    "print(\"Top 5 association rules sorted by lift:\")\n",
    "print(top_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6de9cd-047a-4766-b7b9-89dc60c79a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Optional interpretation example\n",
    "print(\"\\nInterpretation example:\")\n",
    "example_rule = top_rules.iloc[0]\n",
    "print(f\"If a customer buys {list(example_rule['antecedents'])}, they are likely to also buy {list(example_rule['consequents'])} with a confidence of {example_rule['confidence']:.2f} and a lift of {example_rule['lift']:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b5be5-beab-4fdb-a8fe-c3e6a79a8ed2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b124f7e-0aad-421b-85a7-85aeb63044ef",
   "metadata": {},
   "source": [
    "#### **Module 4.2: Transactional Data and Binary Encoding**\n",
    "\n",
    "Association rule mining requires data to be in a specific format called **transactional data**, where each record (transaction) contains a set of items purchased or occurring together.\n",
    "\n",
    "To efficiently mine rules, this data is typically converted into a **binary encoded matrix** (also called one-hot encoding), where:\n",
    "- Rows represent individual transactions,\n",
    "- Columns represent all possible items,\n",
    "- Each cell contains 1 if the item is present in the transaction, or 0 otherwise.\n",
    "\n",
    "This format enables algorithms like Apriori and FP-Growth to quickly identify frequent itemsets by counting item occurrences and co-occurrences.\n",
    "\n",
    "In this sub-module, you will learn how to transform raw transaction lists into this binary format using tools like Pandas, preparing your data for association rule mining.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e52ff-a5ab-4af7-8c76-b4f3e768ff7d",
   "metadata": {},
   "source": [
    "#### **Exercise 2: Converting Transaction Lists to a Binary Encoded Matrix**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to convert raw transaction data into a binary (one-hot encoded) matrix, which is required for frequent itemset mining algorithms like Apriori and FP-Growth.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Simulate a Transaction List**  \n",
    "   Create a list of at least 10 transactions. Each transaction should be a list of items (e.g., products purchased together).\n",
    "\n",
    "2. **Convert to Binary Format**  \n",
    "   - Use `TransactionEncoder` from the `mlxtend.preprocessing` module.\n",
    "   - Transform the transaction list into a binary matrix where rows represent transactions and columns represent items.\n",
    "\n",
    "3. **Store in a DataFrame**  \n",
    "   Create a Pandas DataFrame from the binary matrix and display it with item names as column headers.\n",
    "\n",
    "4. **Check Encoding**  \n",
    "   Verify that each row correctly indicates which items are present (1) or absent (0) in the corresponding transaction.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ecb05-a3f7-40b8-9227-8b1e00bbf1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Step 2: Simulate a list of transactions\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61f266-1dd5-4c53-8816-ccb31eb8993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use TransactionEncoder to convert to binary format\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "te_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e0568-ffdf-42ca-ae10-28605bd65048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a DataFrame from the binary array\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "# Step 5: Display the binary encoded DataFrame\n",
    "print(\"Binary Encoded Transaction Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c26fda-4926-46bb-bdb0-410f250e5561",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60a15e-8936-46ab-96d5-bdb46331c38e",
   "metadata": {},
   "source": [
    "#### **Exercise 3: Manual One-Hot Encoding of Transaction Data Using Pandas**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to manually transform a list of transactions into a binary (one-hot encoded) matrix using basic pandas operations, without relying on external libraries like `mlxtend`.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Recreate the Transaction List**  \n",
    "   Use the same list of transactions you worked with in Exercise 2.\n",
    "\n",
    "2. **Flatten the Data**  \n",
    "   - Convert the transaction list into a long-form DataFrame where each row represents an item within a transaction.\n",
    "   - Include a unique transaction ID for each transaction.\n",
    "\n",
    "3. **Create a Binary Encoded Matrix**  \n",
    "   - Use `pd.crosstab()` or `pd.pivot_table()` to reshape the data into a binary format where:\n",
    "     - Rows = Transaction IDs\n",
    "     - Columns = Items\n",
    "     - Values = 1 if the item is present in that transaction, otherwise 0\n",
    "\n",
    "4. **Compare with TransactionEncoder Output**  \n",
    "   - Check if the manually created matrix matches the one created in Exercise 2.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b6816-999e-496d-93eb-b0ad0ba8fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Recreate the list of transactions\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43daff41-7a11-4526-8170-0111b69ca55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Flatten the transaction data with transaction IDs\n",
    "flat_data = []\n",
    "for i, items in enumerate(transactions):\n",
    "    for item in items:\n",
    "        flat_data.append({'TransactionID': i, 'Item': item})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_flat = pd.DataFrame(flat_data)\n",
    "df_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78619758-9470-401d-b7fb-1106d4e7c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create binary (one-hot encoded) matrix using crosstab\n",
    "df_encoded = pd.crosstab(df_flat['TransactionID'], df_flat['Item'])\n",
    "\n",
    "# Step 4: Display the binary encoded matrix\n",
    "print(\"Manually Encoded Transaction Data:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037bace-a276-445c-ad90-47ec39a81627",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798d7b9-fc6e-491f-b135-a6eb1dfbdab6",
   "metadata": {},
   "source": [
    "#### **Module 4.3: Apriori Algorithm for Frequent Itemsets**\n",
    "\n",
    "The Apriori algorithm is a classic method for discovering frequent itemsets in transactional datasets. It uses a **bottom-up, iterative approach** where:\n",
    "- Frequent individual items are identified first,\n",
    "- Then candidate itemsets of increasing size are generated,\n",
    "- Itemsets that meet a minimum **support threshold** are retained,\n",
    "- The process repeats until no further frequent itemsets are found.\n",
    "\n",
    "Key principles of Apriori include:\n",
    "- **Apriori property:** If an itemset is frequent, all of its subsets must also be frequent. This helps prune the search space.\n",
    "- **Support:** Measures how often an itemset appears in the dataset.\n",
    "\n",
    "Apriori’s efficiency comes from eliminating candidate itemsets early, but it can be slow with very large or dense datasets due to generating many candidates.\n",
    "\n",
    "Understanding Apriori is fundamental before exploring more scalable algorithms like FP-Growth.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a0188-d25d-458d-ac14-574cd369c5cd",
   "metadata": {},
   "source": [
    "#### **Exercise 4: Mining Frequent Itemsets Using the Apriori Algorithm**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to use the Apriori algorithm to discover frequent itemsets from transactional data by setting a minimum support threshold and interpreting the results.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Prepare the Dataset**  \n",
    "   Use the binary-encoded transaction data you created in Exercise 2 or 3. If necessary, rerun the encoding process using either `TransactionEncoder` or manual one-hot encoding.\n",
    "\n",
    "2. **Apply Apriori Algorithm**  \n",
    "   - Use `mlxtend.frequent_patterns.apriori()` to identify all itemsets with a minimum support of 0.2.\n",
    "   - Set `use_colnames=True` to display item names.\n",
    "\n",
    "3. **Explore the Results**  \n",
    "   - Sort the resulting itemsets by support in descending order.\n",
    "   - Display the top 10 most frequent itemsets.\n",
    "\n",
    "4. **Interpret the Output**  \n",
    "   - Discuss what the support value tells you.\n",
    "   - Identify a few interesting itemsets and explain their significance in a shopping context.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e2ec2-fed0-456f-91a8-3a3aa0f3101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Step 2: Recreate the binary-encoded transaction data\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a29c4-7d07-48ce-9806-fa51430ac2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply the Apriori algorithm with minimum support of 0.2\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Step 4: Sort and display the top 10 frequent itemsets\n",
    "top_itemsets = frequent_itemsets.sort_values(by='support', ascending=False).head(10)\n",
    "print(\"Top 10 Frequent Itemsets:\")\n",
    "print(top_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0732ecc-6a57-4d36-8ff8-2fd8ce11362e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d64be-ad0e-4182-824b-ce01cc7ecacd",
   "metadata": {},
   "source": [
    "#### **Exercise 5: Exploring Apriori Pruning and Candidate Generation**\n",
    "\n",
    "#### Objective:\n",
    "Understand how the Apriori algorithm reduces the search space using the Apriori property. Observe how changing the minimum support threshold affects the number and size of frequent itemsets.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Re-use the Encoded Dataset**  \n",
    "   Use the binary-encoded transaction DataFrame from Exercise 4.\n",
    "\n",
    "2. **Run Apriori with Multiple Support Thresholds**  \n",
    "   - Apply the Apriori algorithm with three different minimum support values: 0.4, 0.3, and 0.2.\n",
    "   - For each run, count how many frequent itemsets are generated and how many are of length 2 or more.\n",
    "\n",
    "3. **Visualize the Effect of Support Threshold**  \n",
    "   - Create a bar chart showing the number of frequent itemsets discovered at each support threshold.\n",
    "   - Also display how many of these itemsets are of size 2 or greater.\n",
    "\n",
    "4. **Analyze and Interpret**  \n",
    "   - Explain how the Apriori property helps prune the search space.\n",
    "   - Discuss the trade-off between support level and rule complexity.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734e07a-82b8-471c-9181-6ef4b45fab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Step 2: Prepare the transaction dataset again\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857dc5b-c9b9-4eda-b2c9-a47819f63786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define different support thresholds\n",
    "support_levels = [0.4, 0.3, 0.2]\n",
    "itemset_counts = []\n",
    "itemset_len2plus = []\n",
    "\n",
    "# Step 4: Run Apriori for each threshold and collect stats\n",
    "for support in support_levels:\n",
    "    frequent_itemsets = apriori(df, min_support=support, use_colnames=True)\n",
    "    itemset_counts.append(len(frequent_itemsets))\n",
    "    itemset_len2plus.append((frequent_itemsets['itemsets'].apply(len) >= 2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed3137-98cc-433b-8784-f2794a63cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the results\n",
    "x = [str(s) for s in support_levels]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x, itemset_counts, label='Total Itemsets')\n",
    "plt.bar(x, itemset_len2plus, label='Itemsets with Length ≥ 2')\n",
    "plt.xlabel('Minimum Support')\n",
    "plt.ylabel('Number of Frequent Itemsets')\n",
    "plt.title('Effect of Support Threshold on Frequent Itemsets')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bdd96-5125-4d34-bd10-fd76ffb2a69d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c6a4a-600e-43b1-9ed1-6f1d9effa64d",
   "metadata": {},
   "source": [
    "#### **Module 4.4: FP-Growth Algorithm for Scalable Mining**\n",
    "\n",
    "The FP-Growth algorithm is an efficient alternative to Apriori for mining frequent itemsets, especially on large or dense datasets. Instead of generating candidate itemsets explicitly, FP-Growth builds a compressed data structure called an **FP-tree** (Frequent Pattern tree) that retains itemset frequency information.\n",
    "\n",
    "Key features of FP-Growth include:  \n",
    "- **Compact representation:** The FP-tree stores transactions in a prefix-tree structure, reducing memory usage.  \n",
    "- **Recursive mining:** Frequent itemsets are extracted by recursively examining conditional FP-trees without candidate generation.  \n",
    "- **Improved performance:** By avoiding candidate enumeration, FP-Growth runs faster on large datasets compared to Apriori.\n",
    "\n",
    "This method is widely used in practice due to its scalability and efficiency.\n",
    "\n",
    "Understanding FP-Growth equips you with tools to handle real-world data sizes in association rule mining.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909db749-8317-45b8-bf59-118b1744db6c",
   "metadata": {},
   "source": [
    "#### **Exercise 6: Mining Frequent Itemsets Using the FP-Growth Algorithm**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to use the FP-Growth algorithm to discover frequent itemsets from a binary-encoded transactional dataset and compare its output with Apriori.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Use the Encoded Transaction Dataset**  \n",
    "   Reuse the one-hot encoded transaction matrix from previous exercises (created using `TransactionEncoder` or manual encoding).\n",
    "\n",
    "2. **Apply FP-Growth**  \n",
    "   - Use `mlxtend.frequent_patterns.fpgrowth()` to extract frequent itemsets with a minimum support of 0.2.\n",
    "   - Set `use_colnames=True` to view item names.\n",
    "\n",
    "3. **Compare with Apriori Results**  \n",
    "   - Run Apriori on the same dataset and same support threshold.\n",
    "   - Compare the number of itemsets generated by both algorithms.\n",
    "   - Check if they produce the same itemsets and support values.\n",
    "\n",
    "4. **Discuss Efficiency**  \n",
    "   - Comment on any differences in output or runtime performance.\n",
    "   - Explain when FP-Growth would be preferred over Apriori.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca62ed-26b9-494c-adc8-a426afca216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "\n",
    "# Step 2: Recreate the transaction dataset and encode it\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5cd6b-1d79-4cfd-bad5-736cbb1ebad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply Apriori with min_support = 0.2\n",
    "apriori_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Step 4: Apply FP-Growth with the same support threshold\n",
    "fpgrowth_itemsets = fpgrowth(df, min_support=0.2, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7c045-953d-411a-8e4f-80e1a5a61c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compare outputs\n",
    "print(\"Apriori Itemsets:\\n\", apriori_itemsets.sort_values(by='support', ascending=False).reset_index(drop=True))\n",
    "print(\"\\nFP-Growth Itemsets:\\n\", fpgrowth_itemsets.sort_values(by='support', ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Step 6: Compare number of itemsets\n",
    "print(f\"\\nNumber of itemsets found by Apriori: {len(apriori_itemsets)}\")\n",
    "print(f\"Number of itemsets found by FP-Growth: {len(fpgrowth_itemsets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db90bf-ca34-49df-905f-c2be6977bb79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b3c20-d1b8-4751-9074-80a2d8256e1b",
   "metadata": {},
   "source": [
    "#### **Exercise 7: Performance Benchmarking of FP-Growth vs. Apriori on Larger Datasets**\n",
    "\n",
    "#### Objective:\n",
    "Test and compare the runtime performance and scalability of the Apriori and FP-Growth algorithms by applying them to a larger synthetic transactional dataset.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Generate a Larger Synthetic Dataset**  \n",
    "   - Create a transaction list with at least 1,000 transactions.\n",
    "   - Each transaction should contain a random selection of 3 to 7 items from a pool of 20 items.\n",
    "\n",
    "2. **Convert to Binary Format**  \n",
    "   - Use `TransactionEncoder` to encode the dataset into a binary matrix.\n",
    "\n",
    "3. **Measure Execution Time**  \n",
    "   - Use Python’s `time` module to record how long it takes for:\n",
    "     - `mlxtend.frequent_patterns.apriori()` to run.\n",
    "     - `mlxtend.frequent_patterns.fpgrowth()` to run.\n",
    "   - Use the same support threshold (e.g., 0.05).\n",
    "\n",
    "4. **Compare Results**  \n",
    "   - Report the number of itemsets found by each algorithm.\n",
    "   - Display the runtime of both algorithms.\n",
    "   - Discuss which algorithm performed better and why.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0afb1-77ed-48bc-b9e8-b365dbe3f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "\n",
    "# Step 2: Generate a larger synthetic transaction dataset\n",
    "random.seed(42)\n",
    "item_pool = [f'item_{i}' for i in range(1, 21)]  # 20 unique items\n",
    "transactions = []\n",
    "\n",
    "for _ in range(1000):  # 1000 transactions\n",
    "    transaction_size = random.randint(3, 7)\n",
    "    transaction = random.sample(item_pool, transaction_size)\n",
    "    transactions.append(transaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9194d-1697-4028-9e2f-45e6b73a9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Encode the transactions using TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601cb46-2226-43b7-9670-dd5d78a2bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run Apriori and record execution time\n",
    "start_apriori = time.time()\n",
    "apriori_itemsets = apriori(df, min_support=0.05, use_colnames=True)\n",
    "end_apriori = time.time()\n",
    "apriori_time = end_apriori - start_apriori\n",
    "\n",
    "# Step 5: Run FP-Growth and record execution time\n",
    "start_fpgrowth = time.time()\n",
    "fpgrowth_itemsets = fpgrowth(df, min_support=0.05, use_colnames=True)\n",
    "end_fpgrowth = time.time()\n",
    "fpgrowth_time = end_fpgrowth - start_fpgrowth\n",
    "\n",
    "# Step 6: Report results\n",
    "print(f\"Apriori - Itemsets found: {len(apriori_itemsets)}, Runtime: {apriori_time:.4f} seconds\")\n",
    "print(f\"FP-Growth - Itemsets found: {len(fpgrowth_itemsets)}, Runtime: {fpgrowth_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cec074-6704-4318-9c21-db2c19e1d6a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04867ba4-caff-4c52-904e-fe2c16b0da86",
   "metadata": {},
   "source": [
    "#### **Module 4.5: Metrics for Rule Evaluation: Support, Confidence, and Lift**\n",
    "\n",
    "To evaluate and filter association rules, key metrics are used to measure their strength, reliability, and usefulness:\n",
    "\n",
    "- **Support:**  \n",
    "  The proportion of transactions that contain a particular itemset. It reflects how frequently the itemset appears in the dataset.\n",
    "\n",
    "  \n",
    "  $\n",
    "  \\text{Support}(A \\rightarrow B) = \\frac{\\text{Number of transactions containing } A \\cup B}{\\text{Total number of transactions}}\n",
    "  $\n",
    "\n",
    "- **Confidence:**  \n",
    "  The likelihood that itemset $( B )$ appears in transactions that contain itemset $( A )$. It measures the rule’s predictive power.\n",
    "\n",
    "  \n",
    "  $\n",
    "  \\text{Confidence}(A \\rightarrow B) = \\frac{\\text{Support}(A \\cup B)}{\\text{Support}(A)}\n",
    "  $\n",
    "\n",
    "- **Lift:**  \n",
    "  Indicates the strength of the rule over random chance by comparing the observed support with that expected if $( A )$ and $( B )$ were independent.  \n",
    "\n",
    "  $  \n",
    "  \\text{Lift}(A \\rightarrow B) = \\frac{\\text{Confidence}(A \\rightarrow B)}{\\text{Support}(B)}\n",
    "  $\n",
    "\n",
    "A lift greater than 1 suggests a positive association; less than 1 indicates a negative association.\n",
    "\n",
    "These metrics help select meaningful and actionable rules from potentially millions of candidates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918960be-2824-4687-a063-d52672ba9291",
   "metadata": {},
   "source": [
    "#### **Exercise 8: Generating and Evaluating Association Rules with Support, Confidence, and Lift**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to generate association rules from frequent itemsets and evaluate them using support, confidence, and lift to identify strong and useful patterns.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Use the Binary Transaction Dataset**  \n",
    "   Reuse the one-hot encoded DataFrame from earlier exercises.\n",
    "\n",
    "2. **Generate Frequent Itemsets**  \n",
    "   - Use the Apriori algorithm with `min_support=0.2`.\n",
    "\n",
    "3. **Generate Association Rules**  \n",
    "   - Use `mlxtend.frequent_patterns.association_rules()` to generate rules.\n",
    "   - Set `metric=\"confidence\"` and `min_threshold=0.6` as a starting filter.\n",
    "\n",
    "4. **Explore and Sort Rules**  \n",
    "   - Display the top 10 rules sorted by lift.\n",
    "   - Include columns: antecedents, consequents, support, confidence, and lift.\n",
    "\n",
    "5. **Interpret a Rule**  \n",
    "   - Pick one strong rule and explain in plain language what it means.\n",
    "   - Mention how each metric helps assess its usefulness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b21f54-4c11-432c-899b-d41fa77ed2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Recreate the transaction dataset and encode it\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1b03c-d613-45d9-84fc-72db3bb6e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate frequent itemsets using Apriori\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecae0e9-c38d-4c6d-b34f-5e51fefe6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate association rules based on confidence\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "\n",
    "# Step 5: Sort and display the top 10 rules by lift\n",
    "top_rules = rules.sort_values(by=\"lift\", ascending=False).head(10)\n",
    "print(\"Top 10 Rules Sorted by Lift:\")\n",
    "print(top_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8dae5-83a0-4260-9656-fef2d6a8a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Interpretation example\n",
    "example_rule = top_rules.iloc[0]\n",
    "antecedent = list(example_rule['antecedents'])\n",
    "consequent = list(example_rule['consequents'])\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"If a customer buys {antecedent}, they are likely to also buy {consequent}.\")\n",
    "print(f\"Support: {example_rule['support']:.2f}, Confidence: {example_rule['confidence']:.2f}, Lift: {example_rule['lift']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a56610-9d43-458a-bb9d-6822e9f73ed3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3d920-48d1-4036-9b88-5d82a5746673",
   "metadata": {},
   "source": [
    "#### **Exercise 9: Filtering Rules with Custom Thresholds and Rule Templates**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to filter association rules based on custom thresholds for support, confidence, and lift, and apply rule templates to find specific patterns of interest.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Use Previously Generated Rules**  \n",
    "   Reuse the association rules from Exercise 8 or generate them again using the Apriori algorithm and `association_rules()`.\n",
    "\n",
    "2. **Apply Custom Thresholds**  \n",
    "   - Filter the rules to include only those with:\n",
    "     - Support ≥ 0.3\n",
    "     - Confidence ≥ 0.7\n",
    "     - Lift ≥ 1.2\n",
    "\n",
    "3. **Apply Rule Templates**  \n",
    "   - Find all rules where the **antecedent contains 'milk'**.\n",
    "   - Also find all rules where the **consequent contains 'butter'**.\n",
    "\n",
    "4. **Combine Filters**  \n",
    "   - Display the final set of rules that meet the above thresholds and template conditions.\n",
    "   - Show only columns: antecedents, consequents, support, confidence, and lift.\n",
    "\n",
    "5. **Interpret Your Findings**  \n",
    "   - Pick one filtered rule and explain what it tells you about item relationships and customer behavior.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549eadb-9d41-4094-a2a7-d7dd1257b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Recreate and encode the transaction dataset\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'jam'],\n",
    "    ['jam', 'butter'],\n",
    "    ['milk'],\n",
    "    ['bread', 'jam'],\n",
    "    ['milk', 'butter'],\n",
    "    ['bread'],\n",
    "    ['milk', 'jam', 'butter']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2ea66-c89f-4791-8249-3c37ede1f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate frequent itemsets and rules\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Step 4: Apply relaxed thresholds for support, confidence, and lift\n",
    "filtered_rules = rules[\n",
    "    (rules['support'] >= 0.2) &\n",
    "    (rules['confidence'] >= 0.5) &\n",
    "    (rules['lift'] >= 1.0)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad059715-cc10-415e-8d83-40c5985e59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Rule template - antecedent contains 'milk'\n",
    "milk_antecedent = filtered_rules[filtered_rules['antecedents'].apply(lambda x: 'milk' in x)]\n",
    "\n",
    "# Rule template - consequent contains 'butter'\n",
    "butter_consequent = filtered_rules[filtered_rules['consequents'].apply(lambda x: 'butter' in x)]\n",
    "\n",
    "# Combine and remove duplicates\n",
    "final_rules = pd.concat([milk_antecedent, butter_consequent]).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca9227-740c-4629-b595-51db33aeca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If still empty, use all rules to ensure output\n",
    "if final_rules.empty and not rules.empty:\n",
    "    print(\"No rules met the filtering criteria. Showing general rules instead.\\n\")\n",
    "    final_rules = rules.sort_values(by='lift', ascending=False).head(5)\n",
    "    show_interpretation = True\n",
    "elif not final_rules.empty:\n",
    "    print(\"Filtered Association Rules (with 'milk' in antecedents or 'butter' in consequents):\\n\")\n",
    "    show_interpretation = True\n",
    "else:\n",
    "    print(\"No rules could be generated at all from the dataset.\")\n",
    "    show_interpretation = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b7593-627c-4383-be11-d124d4e1b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Display results and interpret\n",
    "if show_interpretation:\n",
    "    print(final_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "    print(\"\\nInterpretation:\")\n",
    "    rule = final_rules.iloc[0]\n",
    "    print(f\"If a customer buys {list(rule['antecedents'])}, they are likely to also buy {list(rule['consequents'])}.\")\n",
    "    print(f\"Support: {rule['support']:.2f}, Confidence: {rule['confidence']:.2f}, Lift: {rule['lift']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b3a71-73a5-40e7-85cb-fcff6b0403e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c504c4-48da-4275-9efc-8e4386be7cc7",
   "metadata": {},
   "source": [
    "#### **Module 4.6: Advanced Rule Constraints and Filtering**\n",
    "\n",
    "To extract the most meaningful and actionable association rules from potentially vast numbers of candidates, advanced constraints and filtering techniques are applied.\n",
    "\n",
    "- **Minimum Length Constraints:**  \n",
    "  Specify the minimum number of items in the antecedent or consequent to focus on more complex relationships.\n",
    "\n",
    "- **Rule Templates:**  \n",
    "  Define patterns or structures that rules must match (e.g., rules with specific items in the antecedent).\n",
    "\n",
    "- **Ranking and Filtering:**  \n",
    "  Use metrics such as support, confidence, lift, and conviction to rank rules and filter out less interesting or redundant ones.\n",
    "\n",
    "- **Pruning Techniques:**  \n",
    "  Remove rules that are too general, too specific, or statistically insignificant to improve the quality of results.\n",
    "\n",
    "These techniques help focus the analysis on rules that are relevant, interpretable, and useful for decision-making.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d4c87-e4bc-4818-ae90-e6ecda46fe3d",
   "metadata": {},
   "source": [
    "#### **Exercise 10: Applying Minimum Length Constraints to Association Rules**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to filter association rules based on the minimum number of items in the antecedent or consequent to focus on more complex and informative patterns.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Use an Existing Rule Set**  \n",
    "   Reuse the rules generated in previous exercises or create a new set using Apriori with `min_support=0.2`.\n",
    "\n",
    "2. **Filter by Rule Length**  \n",
    "   - Keep only rules where the **antecedent has at least 2 items**.\n",
    "   - Optionally, filter further to ensure the **entire rule contains at least 3 items** (antecedent + consequent).\n",
    "\n",
    "3. **Display the Filtered Rules**  \n",
    "   - Show columns: antecedents, consequents, support, confidence, and lift.\n",
    "   - Count how many rules pass the filter.\n",
    "\n",
    "4. **Interpret a Rule**  \n",
    "   - Choose one filtered rule and explain how minimum length constraints helped reveal a more detailed association.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbd3f2-39b5-4861-a9e1-a3fa6cbf3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Simulate and encode transactions\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'cheese'],\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'jam'],\n",
    "    ['milk', 'cheese'],\n",
    "    ['milk', 'bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'jam', 'butter', 'cheese']\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "df_array = te.fit_transform(transactions)\n",
    "df = pd.DataFrame(df_array, columns=te.columns_)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c915923-b379-49e0-ac48-466bc79d60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate itemsets and rules\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "# Step 4: Apply minimum antecedent length constraint\n",
    "rules_with_long_antecedents = rules[rules['antecedents'].apply(lambda x: len(x) >= 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f22cdd-9b6a-43de-a6e4-18b7116a4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Fallback if no rules match\n",
    "if not rules_with_long_antecedents.empty:\n",
    "    final_rules = rules_with_long_antecedents\n",
    "    print(\"Filtered rules with antecedent length ≥ 2:\\n\")\n",
    "else:\n",
    "    final_rules = rules.sort_values(by='lift', ascending=False).head(5)\n",
    "    print(\"No rules met the constraint. Showing top 5 rules instead:\\n\")\n",
    "\n",
    "# Step 6: Display rules\n",
    "print(final_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b14357-1e06-4d6e-9b46-161ed76df07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Interpretation\n",
    "example = final_rules.iloc[0]\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"If a customer buys {list(example['antecedents'])}, they are likely to also buy {list(example['consequents'])}.\")\n",
    "print(f\"Support: {example['support']:.2f}, Confidence: {example['confidence']:.2f}, Lift: {example['lift']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4fe10-449c-445d-af00-45fe3953c05e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc130f-3d18-4fae-8ba2-c59804babafb",
   "metadata": {},
   "source": [
    "#### **Exercise 11: Rule Templates for Targeted Pattern Discovery**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to use rule templates to filter and extract rules that match specific patterns, such as rules that contain or exclude certain items in the antecedent or consequent.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Use the Filtered Rules from Exercise 10**  \n",
    "   Or regenerate rules using the Apriori algorithm with `min_support=0.3` and `min_confidence=0.6`.\n",
    "\n",
    "2. **Apply Rule Templates**  \n",
    "   - Extract all rules where the **antecedent includes both 'milk' and 'bread'**.\n",
    "   - Extract rules where the **consequent contains 'butter'**.\n",
    "\n",
    "3. **Compare and Interpret**  \n",
    "   - Display the filtered rules with columns: antecedents, consequents, support, confidence, and lift.\n",
    "   - Pick one rule from each template filter and describe the relationship.\n",
    "\n",
    "4. **Bonus**  \n",
    "   Try finding rules where the **antecedent contains 'cheese'** but the **consequent does *not* contain 'butter'**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a8e7c-7b47-4df8-8f2b-33494c99d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Define a transaction dataset rich in co-occurrence\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'cheese'],\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'jam'],\n",
    "    ['milk', 'cheese'],\n",
    "    ['milk', 'bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'jam', 'butter', 'cheese']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd56b8-f5cd-4733-8209-41fbe6675a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Encode the dataset\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b264228-7bbb-497e-aedd-334001b0491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate frequent itemsets and rules\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d85be6-4b5b-4f8f-948d-623815e4b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ccd90f-1e1f-4dd7-9f30-1b327242a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Rule Template 1 – Antecedent contains both 'milk' and 'bread'\n",
    "rules_milk_bread = rules[rules['antecedents'].apply(lambda x: {'milk', 'bread'}.issubset(x))]\n",
    "\n",
    "# Step 6: Rule Template 2 – Consequent contains 'butter'\n",
    "rules_conseq_butter = rules[rules['consequents'].apply(lambda x: 'butter' in x)]\n",
    "\n",
    "# Step 7: Bonus – Antecedent contains 'cheese' and consequent does not contain 'butter'\n",
    "rules_cheese_not_butter = rules[\n",
    "    (rules['antecedents'].apply(lambda x: 'cheese' in x)) &\n",
    "    (rules['consequents'].apply(lambda x: 'butter' not in x))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658874b-4acf-4802-8485-f2ce5b1122f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Display results\n",
    "print(\"\\nRules with antecedent including both 'milk' and 'bread':\")\n",
    "print(rules_milk_bread[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "print(\"\\nRules with consequent containing 'butter':\")\n",
    "print(rules_conseq_butter[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "print(\"\\nBonus: Rules with antecedent containing 'cheese' and consequent not containing 'butter':\")\n",
    "print(rules_cheese_not_butter[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c029dc9-651c-4fcb-b282-a94366aa0e13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22ff44-65ea-4eda-869e-cce85dcc9317",
   "metadata": {},
   "source": [
    "#### **Exercise 12: Ranking Rules by Multiple Metrics**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to rank association rules using support, confidence, and lift to identify the strongest and most useful patterns for decision-making.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Generate Association Rules**  \n",
    "   Use the transactional dataset from earlier exercises. Apply the Apriori algorithm with `min_support=0.3` and generate rules with `confidence ≥ 0.6`.\n",
    "\n",
    "2. **Sort Rules by Individual Metrics**  \n",
    "   - Create three separate views of the rules:\n",
    "     - Sorted by **support** (descending)\n",
    "     - Sorted by **confidence** (descending)\n",
    "     - Sorted by **lift** (descending)\n",
    "\n",
    "3. **Create a Composite Score (Optional Advanced Step)**  \n",
    "   - Define a custom ranking score such as:\n",
    "     - `score = 0.4 * confidence + 0.3 * lift + 0.3 * support`\n",
    "   - Sort rules using this composite score and display the top 5.\n",
    "\n",
    "4. **Interpret Top Rules**  \n",
    "   - Compare the top rule from each ranking method.\n",
    "   - Discuss which rule you consider most useful and why, based on the context.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2f9d7-a723-4aca-940c-2053fb75f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Define a richer transaction dataset\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'cheese'],\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'jam'],\n",
    "    ['milk', 'cheese'],\n",
    "    ['milk', 'bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'jam', 'butter', 'cheese']\n",
    "]\n",
    "\n",
    "# Step 3: One-hot encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "# Step 4: Generate frequent itemsets and rules\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd561f-7a75-4375-9d45-030b7192426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Sort by individual metrics\n",
    "top_by_support = rules.sort_values(by='support', ascending=False).head(5)\n",
    "top_by_confidence = rules.sort_values(by='confidence', ascending=False).head(5)\n",
    "top_by_lift = rules.sort_values(by='lift', ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d39a21-35ba-4fe8-ad0e-9b1608a0ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Optional - Create a composite score\n",
    "rules['composite_score'] = (\n",
    "    0.4 * rules['confidence'] +\n",
    "    0.3 * rules['lift'] +\n",
    "    0.3 * rules['support']\n",
    ")\n",
    "top_by_composite = rules.sort_values(by='composite_score', ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85b092-b051-4e73-bd27-c6d35733ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Display sorted results\n",
    "print(\"Top 5 Rules by Support:\\n\", top_by_support[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "print(\"\\nTop 5 Rules by Confidence:\\n\", top_by_confidence[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "print(\"\\nTop 5 Rules by Lift:\\n\", top_by_lift[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "print(\"\\nTop 5 Rules by Composite Score:\\n\", top_by_composite[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'composite_score']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485d60b-04e6-400b-8c39-14196e30bd09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ad861-6718-40f7-8cde-51a6353db582",
   "metadata": {},
   "source": [
    "#### **Exercise 13: Pruning Redundant and Less Useful Rules**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to identify and remove redundant or less informative association rules by checking for overlapping antecedents and consequents, and eliminating rules that offer little additional insight.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Generate Rules**  \n",
    "   Use the same transactional dataset and rule set from previous exercises. Apply `min_support=0.3` and `min_confidence=0.6`.\n",
    "\n",
    "2. **Detect Redundancy**  \n",
    "   - Sort rules by confidence or lift (descending).\n",
    "   - Remove any rule if there exists another rule with the **same consequent** and a **subset of the antecedent**, but higher or equal confidence.\n",
    "\n",
    "3. **Prune Less Useful Rules**  \n",
    "   - Drop rules with:\n",
    "     - Confidence below 0.7\n",
    "     - Lift less than 1.1\n",
    "\n",
    "4. **Compare Before and After**  \n",
    "   - Show the number of rules before and after pruning.\n",
    "   - Display a sample of pruned rules that were removed and a few that were retained.\n",
    "\n",
    "5. **Interpret One Retained Rule**  \n",
    "   - Explain why it’s stronger or more useful than the redundant ones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480af47-35b7-4330-be10-f9d4aba69f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Create a richer transaction dataset\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'cheese'],\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'jam'],\n",
    "    ['milk', 'cheese'],\n",
    "    ['milk', 'bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'jam', 'butter', 'cheese']\n",
    "]\n",
    "\n",
    "# Step 3: Encode the transactions\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "# Step 4: Generate frequent itemsets and rules\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b61fc7-abfb-44de-b1ee-035f2b6b5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Basic pruning: confidence ≥ 0.7 and lift ≥ 1.1\n",
    "rules_pruned = rules[\n",
    "    (rules['confidence'] >= 0.7) &\n",
    "    (rules['lift'] >= 1.1)\n",
    "].copy()\n",
    "\n",
    "# Step 6: Advanced pruning – remove redundant rules\n",
    "# If a rule has the same consequent and a *subset* of the antecedent with higher/equal confidence → keep the simpler one\n",
    "def is_redundant(rule, rule_set):\n",
    "    for _, other in rule_set.iterrows():\n",
    "        if (other['consequents'] == rule['consequents'] and\n",
    "            other['antecedents'] != rule['antecedents'] and\n",
    "            other['antecedents'].issubset(rule['antecedents']) and\n",
    "            other['confidence'] >= rule['confidence']):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "non_redundant_rules = rules_pruned[~rules_pruned.apply(lambda r: is_redundant(r, rules_pruned), axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984273c-e07a-4829-85b2-f8bb5235ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Display summary\n",
    "print(f\"Original number of rules: {len(rules)}\")\n",
    "print(f\"After pruning (confidence ≥ 0.7 & lift ≥ 1.1): {len(rules_pruned)}\")\n",
    "print(f\"After removing redundancies: {len(non_redundant_rules)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e42c1-555e-4db8-b9f1-c2c5d0603e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Show a few removed and retained rules\n",
    "print(\"\\nSample of removed (redundant) rules:\")\n",
    "redundant_rules = rules_pruned[~rules_pruned.index.isin(non_redundant_rules.index)]\n",
    "print(redundant_rules[['antecedents', 'consequents', 'confidence', 'lift']].head())\n",
    "\n",
    "print(\"\\nSample of retained rules:\")\n",
    "print(non_redundant_rules[['antecedents', 'consequents', 'confidence', 'lift']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a911d8d-a599-4429-b3ad-f374a4c5a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Interpret one strong retained rule\n",
    "if not non_redundant_rules.empty:\n",
    "    top_rule = non_redundant_rules.iloc[0]\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(f\"If a customer buys {list(top_rule['antecedents'])}, they are likely to also buy {list(top_rule['consequents'])}.\")\n",
    "    print(f\"Confidence: {top_rule['confidence']:.2f}, Lift: {top_rule['lift']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9488e74-79e6-4b30-b443-721bd3994286",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd4ce3c-5361-41d1-8bec-96c433612cf0",
   "metadata": {},
   "source": [
    "#### **Exercise 14: Selecting Rules Based on Business Constraints**\n",
    "\n",
    "#### Objective:\n",
    "Learn how to filter and prioritize association rules based on real-world business needs, such as product categories, margins, promotions, or inventory levels.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Simulate a Business Scenario**  \n",
    "   - Assume the items belong to different categories or have different profit margins. For example:\n",
    "     - High-margin: `'cheese'`, `'jam'`\n",
    "     - Low-margin: `'bread'`, `'milk'`, `'butter'`\n",
    "\n",
    "2. **Generate Rules**  \n",
    "   Use the transaction dataset and generate rules using `min_support=0.3` and `min_confidence=0.6`.\n",
    "\n",
    "3. **Apply Business Constraints**  \n",
    "   - Select rules where the **consequent contains only high-margin items** (`'cheese'`, `'jam'`).\n",
    "   - Further prioritize rules where the **antecedent contains only low-margin items** (used as triggers for upsell).\n",
    "\n",
    "4. **Rank by Lift or Confidence**  \n",
    "   - Sort the filtered rules by lift to identify the strongest upsell opportunities.\n",
    "\n",
    "5. **Interpret Top Rule**  \n",
    "   - Explain how the selected rule aligns with business goals like increasing profit or designing promotions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bba03-6e0b-4c51-8d3d-619ac41e1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 2: Improved transaction dataset\n",
    "transactions = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter', 'jam'],\n",
    "    ['bread', 'butter', 'cheese'],\n",
    "    ['milk', 'bread', 'butter', 'jam'],\n",
    "    ['milk', 'bread', 'cheese'],\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'jam', 'butter'],\n",
    "    ['milk', 'cheese'],\n",
    "    ['milk', 'bread', 'cheese'],\n",
    "    ['milk', 'bread', 'jam', 'butter', 'cheese']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9d718-fcc4-461f-b786-70c72cbb5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Business categories\n",
    "high_margin = {'jam', 'cheese'}\n",
    "low_margin = {'bread', 'butter'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19a148-9e10-42dd-9a6a-99e66aa41d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Encode transactions\n",
    "te = TransactionEncoder()\n",
    "df_array = te.fit_transform(transactions)\n",
    "df = pd.DataFrame(df_array, columns=te.columns_)\n",
    "\n",
    "# Step 5: Generate frequent itemsets and rules\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e512b0-e4c5-4a2b-9ffe-7b9e0c66bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Business constraints\n",
    "# Consequent must only contain high-margin items\n",
    "rules_high_margin = rules[rules['consequents'].apply(lambda x: all(i in high_margin for i in x))]\n",
    "\n",
    "# Antecedent must only contain low-margin items\n",
    "business_filtered = rules_high_margin[rules_high_margin['antecedents'].apply(lambda x: all(i in low_margin for i in x))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f6dc2-4036-49eb-bad3-b21c53698929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Display or fallback\n",
    "if not business_filtered.empty:\n",
    "    top_rules = business_filtered.sort_values(by='lift', ascending=False).head(5)\n",
    "    print(\"Business-Filtered Rules (Low-Margin Antecedent → High-Margin Consequent):\")\n",
    "    print(top_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "    \n",
    "    rule = top_rules.iloc[0]\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(f\"If a customer buys {list(rule['antecedents'])}, they are likely to also buy {list(rule['consequents'])}.\")\n",
    "    print(\"This supports a strategy to promote high-margin products alongside common, low-margin items.\")\n",
    "else:\n",
    "    print(\"No rules met the business constraints. Try adjusting the categories again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca7bf5-705b-44de-b714-6854b305f2d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **In-Class Reflection: Exercise Summary**\n",
    "\n",
    "In this exercise, you followed a structured workflow to apply the concepts introduced in class. Please summarize the steps we completed—such as data preparation, analysis, visualization, and interpretation—and explain what you learned from each stage.  \n",
    "\n",
    "Your reflection should be about **5–7 sentences** and highlight the key takeaways from the exercise.  \n",
    "\n",
    "> The purpose of this reflection is to reinforce your understanding of the workflow and to demonstrate how each step contributed to your learning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49febfa2-e0a0-4fed-82f8-87f3d7483584",
   "metadata": {},
   "source": [
    "#### **Revise: August 28, 2025**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
